{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold, GroupShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import os\n",
    "from scipy.stats import spearmanr\n",
    "from math import floor, ceil\n",
    "from transformers import *\n",
    "from sklearn.utils import shuffle\n",
    "from collections import defaultdict\n",
    "import html\n",
    "import gc\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape = (6079, 41)\n",
      "test shape = (476, 11)\n",
      "\n",
      "output categories:\n",
      "\t ['question_asker_intent_understanding', 'question_body_critical', 'question_conversational', 'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer', 'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent', 'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice', 'question_type_compare', 'question_type_consequence', 'question_type_definition', 'question_type_entity', 'question_type_instructions', 'question_type_procedure', 'question_type_reason_explanation', 'question_type_spelling', 'question_well_written', 'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance', 'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure', 'answer_type_reason_explanation', 'answer_well_written']\n",
      "\n",
      "input categories:\n",
      "\t ['question_title', 'question_body', 'answer', 'category']\n"
     ]
    }
   ],
   "source": [
    "PATH = '../input/google-quest-challenge/'\n",
    "BERT_PATH = '../input/bert-base-uncased-huggingface-transformer/'\n",
    "BERT_WEIGHTS_PATH = '../input/bertv3/'\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_PATH+'bert-base-uncased-vocab.txt')\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "\n",
    "df_train = pd.read_csv(PATH+'train.csv')\n",
    "df_test = pd.read_csv(PATH+'test.csv')\n",
    "df_sub = pd.read_csv(PATH+'sample_submission.csv')\n",
    "print('train shape =', df_train.shape)\n",
    "print('test shape =', df_test.shape)\n",
    "\n",
    "df_train.question_body = df_train.question_body.apply(html.unescape)\n",
    "df_train.answer        = df_train.answer.apply(html.unescape)\n",
    "df_test.question_body = df_test.question_body.apply(html.unescape)\n",
    "df_test.answer        = df_test.answer.apply(html.unescape)\n",
    "\n",
    "\n",
    "output_categories = list(df_train.columns[11:])\n",
    "input_categories = list(df_train.columns[[1, 2, 5, 9]])\n",
    "print('\\noutput categories:\\n\\t', output_categories)\n",
    "print('\\ninput categories:\\n\\t', input_categories)\n",
    "\n",
    "df_train.question_body = df_train.question_body.apply(html.unescape)\n",
    "df_train.answer        = df_train.answer.apply(html.unescape)\n",
    "df_test.question_body = df_test.question_body.apply(html.unescape)\n",
    "df_test.answer        = df_test.answer.apply(html.unescape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_to_transformer_inputs(title, question, answer, tokenizer, max_sequence_length):\n",
    "    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n",
    "    \n",
    "    def return_id(str1, str2, truncation_strategy, length):\n",
    "\n",
    "        inputs = tokenizer.encode_plus(str1, str2,\n",
    "            add_special_tokens=True,\n",
    "            max_length=length,\n",
    "            truncation_strategy=truncation_strategy)\n",
    "        \n",
    "        input_ids =  inputs[\"input_ids\"]\n",
    "        input_masks = [1] * len(input_ids)\n",
    "        input_segments = inputs[\"token_type_ids\"]\n",
    "        padding_length = length - len(input_ids)\n",
    "        padding_id = tokenizer.pad_token_id\n",
    "        input_ids = input_ids + ([padding_id] * padding_length)\n",
    "        input_masks = input_masks + ([0] * padding_length)\n",
    "        input_segments = input_segments + ([0] * padding_length)\n",
    "        \n",
    "        return [input_ids, input_masks, input_segments]\n",
    "    \n",
    "    input_ids_q, input_masks_q, input_segments_q = return_id(\n",
    "        title + ' ' + question, None, 'longest_first', max_sequence_length)\n",
    "    \n",
    "    input_ids_qa, input_masks_qa, input_segments_qa = return_id(\n",
    "        title + ' ' + question, answer, 'longest_first', max_sequence_length)\n",
    "    \n",
    "    # input_ids_a, input_masks_a, input_segments_a = return_id(\n",
    "    #     answer, None, 'longest_first', max_sequence_length)\n",
    "    \n",
    "    return [input_ids_q, input_masks_q, input_segments_q,\n",
    "            input_ids_qa, input_masks_qa, input_segments_qa,]\n",
    "            # input_ids_a, input_masks_a, input_segments_a]\n",
    "\n",
    "def compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n",
    "    input_ids_q, input_masks_q, input_segments_q = [], [], []\n",
    "    input_ids_qa, input_masks_qa, input_segments_qa = [], [], []\n",
    "    input_categories = []\n",
    "    \n",
    "    for _, instance in df[columns].iterrows():\n",
    "        t, q, a, c = instance.question_title, instance.question_body, instance.answer, instance.category\n",
    "\n",
    "        ids_q, masks_q, segments_q, ids_qa, masks_qa, segments_qa = \\\n",
    "        _convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n",
    "        \n",
    "        input_ids_q.append(ids_q)\n",
    "        input_masks_q.append(masks_q)\n",
    "        input_segments_q.append(segments_q)\n",
    "\n",
    "        input_ids_qa.append(ids_qa)\n",
    "        input_masks_qa.append(masks_qa)\n",
    "        input_segments_qa.append(segments_qa)\n",
    "        \n",
    "        input_categories.append([c])\n",
    "\n",
    "        # input_ids_a.append(ids_a)\n",
    "        # input_masks_a.append(masks_a)\n",
    "        # input_segments_a.append(segments_a)\n",
    "        \n",
    "    return [np.asarray(input_ids_q, dtype=np.int32), \n",
    "            np.asarray(input_masks_q, dtype=np.int32), \n",
    "            np.asarray(input_segments_q, dtype=np.int32),\n",
    "            np.asarray(input_ids_qa, dtype=np.int32), \n",
    "            np.asarray(input_masks_qa, dtype=np.int32), \n",
    "            np.asarray(input_segments_qa, dtype=np.int32),]\n",
    "            #np.asarray(input_categories, dtype=np.int32),]\n",
    "            # np.asarray(input_ids_a, dtype=np.int32), \n",
    "            # np.asarray(input_masks_a, dtype=np.int32), \n",
    "            # np.asarray(input_segments_a, dtype=np.int32)]\n",
    "\n",
    "def compute_output_arrays(df, columns):\n",
    "    return np.asarray(df[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = compute_output_arrays(df_train, output_categories)\n",
    "inputs = compute_input_arrays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n",
    "test_inputs = compute_input_arrays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PostProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01      , 0.2       , 0.26666667, 0.3       , 0.33333333,\n",
       "       0.33333333, 0.4       , 0.44444444, 0.46666667, 0.5       ,\n",
       "       0.53333333, 0.55555556, 0.6       , 0.66666667, 0.66666667,\n",
       "       0.7       , 0.73333333, 0.77777778, 0.8       , 0.83333333,\n",
       "       0.86666667, 0.88888889, 0.9       , 0.93333333, 0.999999  ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def postprocess_nariman(preds):\n",
    "    preds_temp = preds.copy()\n",
    "\n",
    "    for i in range(preds_temp.shape[0]):\n",
    "        for j in range(preds_temp.shape[1]):\n",
    "            preds_temp[i][j] = min(unique_targets_n, key=lambda x: abs(x - preds_temp[i][j]))\n",
    "    \n",
    "    for i in range(preds.shape[1]):\n",
    "      if len(np.unique(preds_temp[:,i])) < 8:\n",
    "        preds_temp[:,i] = preds[:,i]\n",
    "\n",
    "\n",
    "\n",
    "    return preds_temp\n",
    "\n",
    "def remove_null(preds):\n",
    "    for col in range(preds.shape[1]):\n",
    "        if np.isnan(preds[:, col]).any():\n",
    "            preds[:, col][np.argwhere(np.isnan(preds[:,col]))] = 0.001\n",
    "            \n",
    "    return preds\n",
    "\n",
    "targets = np.zeros(0)\n",
    "for col in range(11, 41):\n",
    "    targets = np.append(targets, df_train.iloc[:, col].unique())\n",
    "\n",
    "unique_targets_n = np.unique(targets)\n",
    "unique_targets_n[unique_targets_n == 1] = 0.999999\n",
    "unique_targets_n[unique_targets_n == 0] = 0.01\n",
    "unique_targets_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PostProcessing Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_targets = np.unique(df_train.iloc[:, 11:])\n",
    "ids = [2, 11, 12, 14, 15]\n",
    "\n",
    "def discretize(preds):\n",
    "    preds_temp = preds.copy()\n",
    "\n",
    "    for j in range(preds_temp.shape[1]):\n",
    "      for i in range(preds_temp.shape[0]):\n",
    "          preds_temp[i, j] = min(unique_targets, key=lambda x: abs(x - preds_temp[i, j]))\n",
    "\n",
    "    for i in range(preds_temp.shape[1]):\n",
    "      if i not in ids:\n",
    "        preds_temp[:, i] = preds[:, i]\n",
    "    \n",
    "    return preds_temp\n",
    "\n",
    "def postprocess_mark(preds):\n",
    "  preds = discretize(preds)\n",
    "  return preds\n",
    "\n",
    "def scale_outputs(outputs):\n",
    "  values = sorted(np.unique(outputs))\n",
    "  uniform_values = np.arange(len(values)) / (len(values) - 1)\n",
    "  return np.array([uniform_values[values.index(output)] for output in outputs])\n",
    "\n",
    "for col in range(30):\n",
    "  outputs[:, col] = scale_outputs(outputs[:, col])\n",
    "\n",
    "\n",
    "n = df_test['url'].apply(lambda x:(('ell.stackexchange.com' in x) or ('english.stackexchange.com' in x))).tolist()\n",
    "spelling = []\n",
    "\n",
    "for x in n:\n",
    "    if x:\n",
    "        spelling.append(1.)\n",
    "    else:\n",
    "        spelling.append(0.)\n",
    "        \n",
    "        \n",
    "def get_spelling_preds(indices):\n",
    "    preds = []\n",
    "    for id in indices:\n",
    "        if 'ell.stackexchange.com' in df_train.iloc[id, 8] or 'english.stackexchange.com' in df_train.iloc[id, 8]:\n",
    "            preds.append(1)\n",
    "        else:\n",
    "            preds.append(0)\n",
    "    return preds\n",
    "\n",
    "def postprocess_quantiles_col(preds, val_idx, col_id):\n",
    "    not_val_idx = [i for i in range(df_train.shape[0]) if i not in val_idx]\n",
    "    train_vals = df_train.iloc[not_val_idx, 11 + col_id]\n",
    "    train_vals_distr = train_vals.value_counts().sort_index() / len(train_vals)\n",
    "    \n",
    "    n_filled = 0\n",
    "    new_preds = preds.copy()\n",
    "    args_sorted = np.argsort(preds)\n",
    "    for val, percentage in train_vals_distr.items():\n",
    "        n_to_fill = int(len(preds) * percentage)\n",
    "        n_to_fill = max(1, n_to_fill)\n",
    "        new_preds[args_sorted[n_filled: n_filled + n_to_fill]] = val\n",
    "        n_filled += n_to_fill\n",
    "    if n_filled < len(preds):\n",
    "        new_preds[n_filled:] = val\n",
    "    return new_preds\n",
    "\n",
    "def postprocess_quantiles(preds, val_idx, cols=[]):\n",
    "    postprocessed = preds.copy()\n",
    "    if cols:\n",
    "        for col in cols:\n",
    "            postprocessed[:, col] = postprocess_quantiles_col(preds[:, col], val_idx, col)\n",
    "    else:\n",
    "        for col in range(30):\n",
    "            postprocessed[:, col] = postprocess_quantiles_col(preds[:, col], val_idx, col)\n",
    "    return postprocessed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spearmanr_ignore_nan(trues, preds):\n",
    "    rhos = []\n",
    "    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n",
    "        rhos.append(spearmanr(tcol, pcol).correlation)\n",
    "    return np.nanmean(rhos), rhos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bert mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_mark_5():\n",
    "    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    qa_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    qa_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    qa_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "\n",
    "    config = BertConfig() # print(config) to see settings\n",
    "    config.output_hidden_states = False # Set to True to obtain hidden states\n",
    "\n",
    "    bert_model = TFBertModel.from_pretrained(\n",
    "      BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n",
    "  \n",
    "    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n",
    "    qa_embedding = bert_model(qa_id, attention_mask=qa_mask, token_type_ids=qa_atn)[0]\n",
    "\n",
    "    q = q_embedding[:, 0, :]#tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n",
    "    qa = qa_embedding[:, 0, :]\n",
    "\n",
    "    q = tf.keras.layers.Dropout(0.2)(q)\n",
    "    qa = tf.keras.layers.Dropout(0.2)(qa)\n",
    "\n",
    "    #qqa = tf.keras.layers.Concatenate(axis=1)([q, qa])\n",
    "\n",
    "    q = tf.keras.layers.Dense(21, activation='sigmoid')(q)\n",
    "    #qqa = tf.keras.layers.Dense(8, activation='sigmoid')(qqa)\n",
    "    qa = tf.keras.layers.Dense(9, activation='sigmoid')(qa)\n",
    "\n",
    "    outputs = tf.keras.layers.Concatenate(axis=1)([q, qa])\n",
    "  \n",
    "    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, qa_id, qa_mask, qa_atn], outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_mark_4():\n",
    "    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    qa_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    qa_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    qa_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "\n",
    "    config = BertConfig() # print(config) to see settings\n",
    "    config.output_hidden_states = False # Set to True to obtain hidden states\n",
    "\n",
    "    bert_model = TFBertModel.from_pretrained(\n",
    "      BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n",
    "  \n",
    "    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n",
    "    qa_embedding = bert_model(qa_id, attention_mask=qa_mask, token_type_ids=qa_atn)[0]\n",
    "\n",
    "    q = q_embedding[:, 0, :]#tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n",
    "    qa = qa_embedding[:, 0, :]\n",
    "\n",
    "    q = tf.keras.layers.Dropout(0.175)(q)\n",
    "    qa = tf.keras.layers.Dropout(0.175)(qa)\n",
    "\n",
    "    #qqa = tf.keras.layers.Concatenate(axis=1)([q, qa])\n",
    "\n",
    "    q = tf.keras.layers.Dense(21, activation='sigmoid')(q)\n",
    "    #qqa = tf.keras.layers.Dense(8, activation='sigmoid')(qqa)\n",
    "    qa = tf.keras.layers.Dense(9, activation='sigmoid')(qa)\n",
    "\n",
    "    outputs = tf.keras.layers.Concatenate(axis=1)([q, qa])\n",
    "  \n",
    "    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, qa_id, qa_mask, qa_atn], outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_mark_3():\n",
    "    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    qa_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    qa_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    qa_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "\n",
    "    config = BertConfig() # print(config) to see settings\n",
    "    config.output_hidden_states = False # Set to True to obtain hidden states\n",
    "\n",
    "    bert_model = TFBertModel.from_pretrained(\n",
    "      BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n",
    "  \n",
    "    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n",
    "    qa_embedding = bert_model(qa_id, attention_mask=qa_mask, token_type_ids=qa_atn)[0]\n",
    "\n",
    "    q = q_embedding[:, 0, :]#tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n",
    "    qa = qa_embedding[:, 0, :]\n",
    "\n",
    "    q = tf.keras.layers.Dropout(0.25)(q)\n",
    "    qa = tf.keras.layers.Dropout(0.25)(qa)\n",
    "\n",
    "    #qqa = tf.keras.layers.Concatenate(axis=1)([q, qa])\n",
    "\n",
    "    q = tf.keras.layers.Dense(21, activation='sigmoid')(q)\n",
    "    #qqa = tf.keras.layers.Dense(8, activation='sigmoid')(qqa)\n",
    "    qa = tf.keras.layers.Dense(9, activation='sigmoid')(qa)\n",
    "\n",
    "    outputs = tf.keras.layers.Concatenate(axis=1)([q, qa])\n",
    "  \n",
    "    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, qa_id, qa_mask, qa_atn], outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_mark_2():\n",
    "    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    qa_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    qa_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    qa_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "\n",
    "    config = BertConfig() # print(config) to see settings\n",
    "    config.output_hidden_states = False # Set to True to obtain hidden states\n",
    "\n",
    "    bert_model = TFBertModel.from_pretrained(\n",
    "      BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n",
    "  \n",
    "    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n",
    "    qa_embedding = bert_model(qa_id, attention_mask=qa_mask, token_type_ids=qa_atn)[0]\n",
    "\n",
    "    q = q_embedding[:, 0, :]#tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n",
    "    qa = qa_embedding[:, 0, :]\n",
    "\n",
    "    q = tf.keras.layers.Dropout(0.2)(q)\n",
    "    qa = tf.keras.layers.Dropout(0.2)(qa)\n",
    "\n",
    "    #qqa = tf.keras.layers.Concatenate(axis=1)([q, qa])\n",
    "\n",
    "    q = tf.keras.layers.Dense(21, activation='sigmoid')(q)\n",
    "    #qqa = tf.keras.layers.Dense(8, activation='sigmoid')(qqa)\n",
    "    qa = tf.keras.layers.Dense(9, activation='sigmoid')(qa)\n",
    "\n",
    "    outputs = tf.keras.layers.Concatenate(axis=1)([q, qa])\n",
    "  \n",
    "    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, qa_id, qa_mask, qa_atn], outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_mark():\n",
    "    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    qa_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    qa_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    qa_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    #category = tf.keras.layers.Input((1,), dtype=tf.int32)\n",
    "\n",
    "    config = BertConfig() # print(config) to see settings\n",
    "    config.output_hidden_states = False # Set to True to obtain hidden states\n",
    "\n",
    "    bert_model = TFBertModel.from_pretrained(\n",
    "      BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n",
    "  \n",
    "    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n",
    "    qa_embedding = bert_model(qa_id, attention_mask=qa_mask, token_type_ids=qa_atn)[0]\n",
    "    #cat_embedding = tf.keras.layers.Embedding(len(le.classes_), 50, input_length=1)(category)\n",
    "    #cat_embedding = tf.keras.layers.Flatten()(cat_embedding)\n",
    "\n",
    "    q = q_embedding[:, 0, :]#tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n",
    "    qa = qa_embedding[:, 0, :]\n",
    "\n",
    "    q = tf.keras.layers.Dropout(0.2)(q)\n",
    "    qa = tf.keras.layers.Dropout(0.2)(qa)\n",
    "    \n",
    "    #q = tf.keras.layers.Concatenate(axis=1)([q, cat_embedding])\n",
    "    #qa = tf.keras.layers.Concatenate(axis=1)([qa, cat_embedding])\n",
    "\n",
    "    #qqa = tf.keras.layers.Concatenate(axis=1)([q, qa])\n",
    "\n",
    "    q = tf.keras.layers.Dense(21, activation='sigmoid')(q)\n",
    "    #qqa = tf.keras.layers.Dense(8, activation='sigmoid')(qqa)\n",
    "    qa = tf.keras.layers.Dense(9, activation='sigmoid')(qa)\n",
    "\n",
    "    outputs = tf.keras.layers.Concatenate(axis=1)([q, qa])\n",
    "  \n",
    "    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, qa_id, qa_mask, qa_atn], outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### bert_ensemble 0-4 Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_0_4():\n",
    "    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "   \n",
    "\n",
    "    config = BertConfig() # print(config) to see settings\n",
    "    config.output_hidden_states = False # Set to True to obtain hidden states\n",
    "    bert_model = TFBertModel.from_pretrained(\n",
    "        BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n",
    "    \n",
    "    # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n",
    "    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n",
    "    a_embedding = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n",
    "    \n",
    "    q = q_embedding[:, 0, :]#tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n",
    "    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n",
    "    \n",
    "    q = tf.keras.layers.Dropout(0.2)(q)\n",
    "    \n",
    "    a = tf.keras.layers.Dropout(0.2)(a)\n",
    "    \n",
    "    qa = tf.keras.layers.Concatenate(axis=1)([q, a])\n",
    "    \n",
    "    q_outputs = tf.keras.layers.Dense(21, activation='sigmoid')(q)\n",
    "    qa_outputs = tf.keras.layers.Dense(8, activation='sigmoid')(qa)\n",
    "    a_outputs = tf.keras.layers.Dense(1, activation='sigmoid')(a)\n",
    "\n",
    "\n",
    "    outputs = tf.keras.layers.Concatenate(axis=1)([q_outputs, qa_outputs, a_outputs])\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, a_id, a_mask, a_atn], outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### bert_ensemble 5-9 Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_5_9():\n",
    "    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "   \n",
    "\n",
    "    \n",
    "    config = BertConfig() # print(config) to see settings\n",
    "    config.output_hidden_states = False # Set to True to obtain hidden states\n",
    "    # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n",
    "    \n",
    "    # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n",
    "    # pretrained model has been downloaded manually and uploaded to kaggle. \n",
    "    bert_model = TFBertModel.from_pretrained(\n",
    "        BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n",
    "    \n",
    "    # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n",
    "    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n",
    "    a_embedding = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n",
    "  \n",
    "    \n",
    "\n",
    "    \n",
    "    q = q_embedding[:, 0, :]#tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n",
    "    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n",
    "    \n",
    "    q = tf.keras.layers.Dropout(0.2)(q)\n",
    "    \n",
    "    a = tf.keras.layers.Dropout(0.2)(a)\n",
    "    \n",
    "    qa = tf.keras.layers.Concatenate(axis=1)([q, a])\n",
    "    \n",
    "    q_outputs = tf.keras.layers.Dense(21, activation='sigmoid')(q)\n",
    "    qa_outputs = tf.keras.layers.Dense(8, activation='sigmoid')(qa)\n",
    "    a_outputs = tf.keras.layers.Dense(1, activation='sigmoid')(a)\n",
    "    \n",
    "\n",
    "    outputs = tf.keras.layers.Concatenate(axis=1)([q_outputs, qa_outputs, a_outputs])\n",
    "    #q_2_id, q_2_mask, q_2_atn, a_2_id, a_2_mask, a_2_atn\n",
    "    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, a_id, a_mask, a_atn], outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### bert_ensemble 10-14 Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_10_14():\n",
    "    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "   \n",
    "    \n",
    "\n",
    "    config = BertConfig() # print(config) to see settings\n",
    "    config.output_hidden_states = False # Set to True to obtain hidden states\n",
    "    # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n",
    "    \n",
    "    # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n",
    "    # pretrained model has been downloaded manually and uploaded to kaggle. \n",
    "    bert_model = TFBertModel.from_pretrained(\n",
    "        BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n",
    "    \n",
    "    # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n",
    "    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n",
    "    a_embedding = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n",
    "  \n",
    "\n",
    "    q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)#q_embedding[:, 0, :]#\n",
    "    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)#a_embedding[:, 0, :]#\n",
    "    \n",
    "    q = tf.keras.layers.Dropout(0.2)(q)\n",
    "    \n",
    "    a = tf.keras.layers.Dropout(0.2)(a)\n",
    "    \n",
    "    qa = tf.keras.layers.Concatenate(axis=1)([q, a])\n",
    "    \n",
    "    q_outputs = tf.keras.layers.Dense(21, activation='sigmoid')(q)\n",
    "    qa_outputs = tf.keras.layers.Dense(9, activation='sigmoid')(qa)\n",
    "    #a_outputs = tf.keras.layers.Dense(1, activation='sigmoid')(a)\n",
    "\n",
    "    outputs = tf.keras.layers.Concatenate(axis=1)([q_outputs, qa_outputs])\n",
    "\n",
    "    #q_2_id, q_2_mask, q_2_atn, a_2_id, a_2_mask, a_2_atn\n",
    "    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, a_id, a_mask, a_atn], outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### bert_ensemble 15-19 Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_15_19():\n",
    "    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "   \n",
    "\n",
    "    \n",
    "    config = BertConfig() # print(config) to see settings\n",
    "    config.output_hidden_states = False # Set to True to obtain hidden states\n",
    "    # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n",
    "    \n",
    "    # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n",
    "    # pretrained model has been downloaded manually and uploaded to kaggle. \n",
    "    bert_model = TFBertModel.from_pretrained(\n",
    "        BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n",
    "    \n",
    "    # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n",
    "    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n",
    "    a_embedding = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n",
    "  \n",
    "    \n",
    "    q = q_embedding[:, 0, :]#tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n",
    "    a = a_embedding[:, 0, :]#tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n",
    "    \n",
    "    q = tf.keras.layers.Dropout(0.2)(q)\n",
    "    \n",
    "    a = tf.keras.layers.Dropout(0.2)(a)\n",
    "    \n",
    "    qa = tf.keras.layers.Concatenate(axis=1)([q, a])\n",
    "    \n",
    "    q_outputs = tf.keras.layers.Dense(21, activation='sigmoid')(q)\n",
    "    qa_outputs = tf.keras.layers.Dense(9, activation='sigmoid')(qa)\n",
    "    #a_outputs = tf.keras.layers.Dense(1, activation='sigmoid')(a)\n",
    "\n",
    "    outputs = tf.keras.layers.Concatenate(axis=1)([q_outputs, qa_outputs])\n",
    "\n",
    "    #q_2_id, q_2_mask, q_2_atn, a_2_id, a_2_mask, a_2_atn\n",
    "    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, a_id, a_mask, a_atn], outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "test_preds = []\n",
    "start = time.time()\n",
    "for i in range(20):\n",
    "    if i > -1 and i < 5:\n",
    "        model_path = f'../input/bert-ensemble/bert-fold{i}.h5'\n",
    "        model = create_model_0_4()\n",
    "        model.load_weights(model_path)\n",
    "    elif i > 4 and i < 10:\n",
    "        model_path = f'../input/bert-ensemble/bert-fold{i}.h5'\n",
    "        model = create_model_5_9()\n",
    "        model.load_weights(model_path)\n",
    "    elif i > 9 and i < 14:\n",
    "        model_path = f'../input/bert-ensemble/bert-fold{i}.h5'\n",
    "        model = create_model_10_14()\n",
    "        model.load_weights(model_path)\n",
    "    else:\n",
    "        model_path = f'../input/bert-ensemble/bert-fold{i}.h5'\n",
    "        model = create_model_15_19()\n",
    "        model.load_weights(model_path)\n",
    "    \n",
    "    \n",
    "  \n",
    "    print(str(i) + ' - model')\n",
    "    print('Weights loaded: ', time.time() - start)\n",
    "    test_predictions = model.predict(test_inputs)\n",
    "    \n",
    "    test_preds.append(test_predictions)\n",
    "\n",
    "    del model\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "  \n",
    "    print(\"Predictions calculated:\", time.time() - start)\n",
    "    print('----------------------------------')\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference on Mark`s models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights loaded:  19.997506141662598\n",
      "Predictions calculated: 46.374995470047\n",
      "----------------------------------\n",
      "Weights loaded:  54.76851749420166\n",
      "Predictions calculated: 80.27562355995178\n",
      "----------------------------------\n",
      "Weights loaded:  89.20894050598145\n",
      "Predictions calculated: 114.17156982421875\n",
      "----------------------------------\n",
      "Weights loaded:  123.38343405723572\n",
      "Predictions calculated: 147.95179748535156\n",
      "----------------------------------\n",
      "Weights loaded:  156.76055216789246\n",
      "Predictions calculated: 181.5310938358307\n",
      "----------------------------------\n",
      "Weights loaded:  190.44049191474915\n",
      "Predictions calculated: 215.29428839683533\n",
      "----------------------------------\n",
      "Weights loaded:  223.10839128494263\n",
      "Predictions calculated: 247.9885528087616\n",
      "----------------------------------\n",
      "Weights loaded:  256.18144965171814\n",
      "Predictions calculated: 281.23700523376465\n",
      "----------------------------------\n",
      "Weights loaded:  289.4767451286316\n",
      "Predictions calculated: 314.2036015987396\n",
      "----------------------------------\n",
      "Weights loaded:  322.98023104667664\n",
      "Predictions calculated: 348.47482919692993\n",
      "----------------------------------\n",
      "Weights loaded:  357.9386842250824\n",
      "Predictions calculated: 382.80515575408936\n",
      "----------------------------------\n",
      "Weights loaded:  391.86000084877014\n",
      "Predictions calculated: 416.9239983558655\n",
      "----------------------------------\n",
      "Weights loaded:  426.07377767562866\n",
      "Predictions calculated: 451.37332105636597\n",
      "----------------------------------\n",
      "Weights loaded:  460.32107162475586\n",
      "Predictions calculated: 485.49027132987976\n",
      "----------------------------------\n",
      "Weights loaded:  494.91966438293457\n",
      "Predictions calculated: 520.7231950759888\n",
      "----------------------------------\n",
      "Weights loaded:  529.972784280777\n",
      "Predictions calculated: 555.4405648708344\n",
      "----------------------------------\n",
      "Weights loaded:  564.6315505504608\n",
      "Predictions calculated: 590.7596955299377\n",
      "----------------------------------\n",
      "Weights loaded:  599.6206722259521\n",
      "Predictions calculated: 624.7093441486359\n",
      "----------------------------------\n",
      "Weights loaded:  633.5563158988953\n",
      "Predictions calculated: 658.1269533634186\n",
      "----------------------------------\n",
      "Weights loaded:  667.2066841125488\n",
      "Predictions calculated: 692.1461379528046\n",
      "----------------------------------\n",
      "Weights loaded:  701.1675732135773\n",
      "Predictions calculated: 725.992436170578\n",
      "----------------------------------\n",
      "Weights loaded:  734.4150869846344\n",
      "Predictions calculated: 759.7902383804321\n",
      "----------------------------------\n",
      "Weights loaded:  767.8947236537933\n",
      "Predictions calculated: 792.5765869617462\n",
      "----------------------------------\n",
      "Weights loaded:  801.3536565303802\n",
      "Predictions calculated: 826.7485115528107\n",
      "----------------------------------\n",
      "Weights loaded:  835.3984844684601\n",
      "Predictions calculated: 860.400514125824\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mark_test_preds = np.zeros((len(df_test), 30))\n",
    "start = time.time()\n",
    "for i in range(5):\n",
    "    \n",
    "    \n",
    "    model = create_model_mark()\n",
    "    model.load_weights( f'../input/bertv3/bert3e-5-{i}fv3.h5')\n",
    "    print('Weights loaded: ', time.time() - start)\n",
    "    mark_test_preds += model.predict(test_inputs) / 25\n",
    "    #mark_test_preds.append(test_predictions)\n",
    "    print(\"Predictions calculated:\", time.time() - start)\n",
    "    print('----------------------------------')\n",
    "    \n",
    "\n",
    "    model = create_model_mark_2()\n",
    "    model.load_weights( f'../input/bert-mark/bert3e-5-{i}f.h5')\n",
    "    print('Weights loaded: ', time.time() - start)\n",
    "    mark_test_preds += model.predict(test_inputs) / 25\n",
    "    #mark_test_preds.append(test_predictions)\n",
    "    print(\"Predictions calculated:\", time.time() - start)\n",
    "    print('----------------------------------')\n",
    "    \n",
    "\n",
    "    \n",
    "    model = create_model_mark_3()\n",
    "    model.load_weights( f'../input/bert-mark2/bert3e-5-{i}f.h5')\n",
    "    print('Weights loaded: ', time.time() - start)\n",
    "    mark_test_preds += model.predict(test_inputs) / 25\n",
    "    #mark_test_preds.append(test_predictions)\n",
    "    print(\"Predictions calculated:\", time.time() - start)\n",
    "    print('----------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "    model = create_model_mark_4()\n",
    "    model.load_weights( f'../input/bert-mark3/bert3e-5-{i}f.h5')\n",
    "    print('Weights loaded: ', time.time() - start)\n",
    "    mark_test_preds += model.predict(test_inputs) / 25\n",
    "    #mark_test_preds.append(test_predictions)\n",
    "    print(\"Predictions calculated:\", time.time() - start)\n",
    "    print('----------------------------------')\n",
    "    \n",
    "    model = create_model_mark_5()\n",
    "    model.load_weights( f'../input/bert-mark4/bert3e-5-{i}f.h5')\n",
    "    print('Weights loaded: ', time.time() - start)\n",
    "    mark_test_preds += model.predict(test_inputs) / 25\n",
    "    #mark_test_preds.append(test_predictions)\n",
    "    print(\"Predictions calculated:\", time.time() - start)\n",
    "    print('----------------------------------')\n",
    "\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    del model\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_nariman = np.average(test_preds, axis=0)\n",
    "\n",
    "final_preds = ((preds_nariman*0.35) + (mark_test_preds*0.65))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "final_preds = postprocess_quantiles(postprocess_mark(final_preds), [], cols=[5, 9])\n",
    "final_preds[:, 19] = spelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #GAVE 0.423\n",
    "# final_preds = preds_mark\n",
    "# final_preds[:, 1] = preds_nariman[:, 1]\n",
    "# final_preds[:, 3] = preds_nariman[:, 3]\n",
    "# final_preds[:, 4] = preds_nariman[:, 4]\n",
    "# final_preds[:, 7] = preds_nariman[:, 7]\n",
    "# final_preds[:, 8] = preds_nariman[:, 8]\n",
    "# final_preds[:, 10] = preds_nariman[:, 10]\n",
    "# final_preds[:, 16] = preds_nariman[:, 16]\n",
    "# final_preds[:, 18] = preds_nariman[:, 18]\n",
    "# final_preds[:, 26] = preds_nariman[:, 26]\n",
    "# final_preds[:, 28] = preds_nariman[:, 28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# for i in range(preds.shape[1]):\n",
    "#     preds[:, i] -= np.amin(preds[:, i])\n",
    "#     preds[:, i] /= np.amax(preds[:, i])\n",
    "df_sub.iloc[:, 1:] = final_preds # for weighted average set weights=[...]\n",
    "\n",
    "df_sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_asker_intent_understanding</th>\n",
       "      <th>question_body_critical</th>\n",
       "      <th>question_conversational</th>\n",
       "      <th>question_expect_short_answer</th>\n",
       "      <th>question_fact_seeking</th>\n",
       "      <th>question_has_commonly_accepted_answer</th>\n",
       "      <th>question_interestingness_others</th>\n",
       "      <th>question_interestingness_self</th>\n",
       "      <th>question_multi_intent</th>\n",
       "      <th>...</th>\n",
       "      <th>question_well_written</th>\n",
       "      <th>answer_helpful</th>\n",
       "      <th>answer_level_of_information</th>\n",
       "      <th>answer_plausible</th>\n",
       "      <th>answer_relevance</th>\n",
       "      <th>answer_satisfaction</th>\n",
       "      <th>answer_type_instructions</th>\n",
       "      <th>answer_type_procedure</th>\n",
       "      <th>answer_type_reason_explanation</th>\n",
       "      <th>answer_well_written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>0.913424</td>\n",
       "      <td>0.474873</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.531127</td>\n",
       "      <td>0.634390</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.544819</td>\n",
       "      <td>0.474622</td>\n",
       "      <td>0.667949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.891284</td>\n",
       "      <td>0.866451</td>\n",
       "      <td>0.405140</td>\n",
       "      <td>0.896921</td>\n",
       "      <td>0.905565</td>\n",
       "      <td>0.767462</td>\n",
       "      <td>0.052794</td>\n",
       "      <td>0.054397</td>\n",
       "      <td>0.879457</td>\n",
       "      <td>0.771410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>0.821273</td>\n",
       "      <td>0.210151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.786500</td>\n",
       "      <td>0.781689</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.315768</td>\n",
       "      <td>0.175063</td>\n",
       "      <td>0.106917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.494101</td>\n",
       "      <td>0.928367</td>\n",
       "      <td>0.465002</td>\n",
       "      <td>0.922247</td>\n",
       "      <td>0.947569</td>\n",
       "      <td>0.849771</td>\n",
       "      <td>0.911952</td>\n",
       "      <td>0.116967</td>\n",
       "      <td>0.096710</td>\n",
       "      <td>0.719807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>0.865052</td>\n",
       "      <td>0.516582</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.728293</td>\n",
       "      <td>0.901685</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.410020</td>\n",
       "      <td>0.238771</td>\n",
       "      <td>0.199325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.840142</td>\n",
       "      <td>0.859721</td>\n",
       "      <td>0.398799</td>\n",
       "      <td>0.878725</td>\n",
       "      <td>0.878636</td>\n",
       "      <td>0.766212</td>\n",
       "      <td>0.099920</td>\n",
       "      <td>0.054283</td>\n",
       "      <td>0.872505</td>\n",
       "      <td>0.742907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132</td>\n",
       "      <td>0.825589</td>\n",
       "      <td>0.175334</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.717820</td>\n",
       "      <td>0.760658</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.300792</td>\n",
       "      <td>0.129486</td>\n",
       "      <td>0.091134</td>\n",
       "      <td>...</td>\n",
       "      <td>0.598941</td>\n",
       "      <td>0.924412</td>\n",
       "      <td>0.492892</td>\n",
       "      <td>0.919292</td>\n",
       "      <td>0.951111</td>\n",
       "      <td>0.857055</td>\n",
       "      <td>0.795927</td>\n",
       "      <td>0.139120</td>\n",
       "      <td>0.649683</td>\n",
       "      <td>0.735062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>0.871580</td>\n",
       "      <td>0.150992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.816128</td>\n",
       "      <td>0.780855</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.459873</td>\n",
       "      <td>0.363910</td>\n",
       "      <td>0.165606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.498874</td>\n",
       "      <td>0.860891</td>\n",
       "      <td>0.486585</td>\n",
       "      <td>0.893704</td>\n",
       "      <td>0.893761</td>\n",
       "      <td>0.791560</td>\n",
       "      <td>0.221989</td>\n",
       "      <td>0.092568</td>\n",
       "      <td>0.632834</td>\n",
       "      <td>0.747930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>9569</td>\n",
       "      <td>0.842239</td>\n",
       "      <td>0.358038</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.772706</td>\n",
       "      <td>0.804677</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.334363</td>\n",
       "      <td>0.163259</td>\n",
       "      <td>0.029743</td>\n",
       "      <td>...</td>\n",
       "      <td>0.586405</td>\n",
       "      <td>0.942469</td>\n",
       "      <td>0.520021</td>\n",
       "      <td>0.929353</td>\n",
       "      <td>0.965514</td>\n",
       "      <td>0.887291</td>\n",
       "      <td>0.935165</td>\n",
       "      <td>0.143282</td>\n",
       "      <td>0.079139</td>\n",
       "      <td>0.719842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>9590</td>\n",
       "      <td>0.824169</td>\n",
       "      <td>0.213743</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.708283</td>\n",
       "      <td>0.791283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.339415</td>\n",
       "      <td>0.174406</td>\n",
       "      <td>0.076199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525921</td>\n",
       "      <td>0.878598</td>\n",
       "      <td>0.446159</td>\n",
       "      <td>0.852091</td>\n",
       "      <td>0.878557</td>\n",
       "      <td>0.805854</td>\n",
       "      <td>0.764479</td>\n",
       "      <td>0.137364</td>\n",
       "      <td>0.155623</td>\n",
       "      <td>0.689073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>9597</td>\n",
       "      <td>0.783036</td>\n",
       "      <td>0.102558</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.709988</td>\n",
       "      <td>0.797641</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.297251</td>\n",
       "      <td>0.146404</td>\n",
       "      <td>0.471745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.579492</td>\n",
       "      <td>0.907477</td>\n",
       "      <td>0.437612</td>\n",
       "      <td>0.896407</td>\n",
       "      <td>0.921612</td>\n",
       "      <td>0.811333</td>\n",
       "      <td>0.531473</td>\n",
       "      <td>0.139557</td>\n",
       "      <td>0.549874</td>\n",
       "      <td>0.789518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>9623</td>\n",
       "      <td>0.912953</td>\n",
       "      <td>0.714113</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.918641</td>\n",
       "      <td>0.918016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.434037</td>\n",
       "      <td>0.285231</td>\n",
       "      <td>0.153474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.871370</td>\n",
       "      <td>0.972082</td>\n",
       "      <td>0.634717</td>\n",
       "      <td>0.966755</td>\n",
       "      <td>0.977656</td>\n",
       "      <td>0.937250</td>\n",
       "      <td>0.131338</td>\n",
       "      <td>0.084272</td>\n",
       "      <td>0.861624</td>\n",
       "      <td>0.849877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>9640</td>\n",
       "      <td>0.900505</td>\n",
       "      <td>0.455958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.841991</td>\n",
       "      <td>0.801891</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.426001</td>\n",
       "      <td>0.298824</td>\n",
       "      <td>0.133008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.700830</td>\n",
       "      <td>0.885800</td>\n",
       "      <td>0.456123</td>\n",
       "      <td>0.894282</td>\n",
       "      <td>0.896635</td>\n",
       "      <td>0.809449</td>\n",
       "      <td>0.604094</td>\n",
       "      <td>0.099218</td>\n",
       "      <td>0.281257</td>\n",
       "      <td>0.672800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>476 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     qa_id  question_asker_intent_understanding  question_body_critical  \\\n",
       "0       39                             0.913424                0.474873   \n",
       "1       46                             0.821273                0.210151   \n",
       "2       70                             0.865052                0.516582   \n",
       "3      132                             0.825589                0.175334   \n",
       "4      200                             0.871580                0.150992   \n",
       "..     ...                                  ...                     ...   \n",
       "471   9569                             0.842239                0.358038   \n",
       "472   9590                             0.824169                0.213743   \n",
       "473   9597                             0.783036                0.102558   \n",
       "474   9623                             0.912953                0.714113   \n",
       "475   9640                             0.900505                0.455958   \n",
       "\n",
       "     question_conversational  question_expect_short_answer  \\\n",
       "0                        0.2                      0.531127   \n",
       "1                        0.0                      0.786500   \n",
       "2                        0.0                      0.728293   \n",
       "3                        0.0                      0.717820   \n",
       "4                        0.0                      0.816128   \n",
       "..                       ...                           ...   \n",
       "471                      0.0                      0.772706   \n",
       "472                      0.0                      0.708283   \n",
       "473                      0.0                      0.709988   \n",
       "474                      0.0                      0.918641   \n",
       "475                      0.0                      0.841991   \n",
       "\n",
       "     question_fact_seeking  question_has_commonly_accepted_answer  \\\n",
       "0                 0.634390                                    0.0   \n",
       "1                 0.781689                                    1.0   \n",
       "2                 0.901685                                    1.0   \n",
       "3                 0.760658                                    1.0   \n",
       "4                 0.780855                                    1.0   \n",
       "..                     ...                                    ...   \n",
       "471               0.804677                                    1.0   \n",
       "472               0.791283                                    1.0   \n",
       "473               0.797641                                    1.0   \n",
       "474               0.918016                                    1.0   \n",
       "475               0.801891                                    1.0   \n",
       "\n",
       "     question_interestingness_others  question_interestingness_self  \\\n",
       "0                           0.544819                       0.474622   \n",
       "1                           0.315768                       0.175063   \n",
       "2                           0.410020                       0.238771   \n",
       "3                           0.300792                       0.129486   \n",
       "4                           0.459873                       0.363910   \n",
       "..                               ...                            ...   \n",
       "471                         0.334363                       0.163259   \n",
       "472                         0.339415                       0.174406   \n",
       "473                         0.297251                       0.146404   \n",
       "474                         0.434037                       0.285231   \n",
       "475                         0.426001                       0.298824   \n",
       "\n",
       "     question_multi_intent  ...  question_well_written  answer_helpful  \\\n",
       "0                 0.667949  ...               0.891284        0.866451   \n",
       "1                 0.106917  ...               0.494101        0.928367   \n",
       "2                 0.199325  ...               0.840142        0.859721   \n",
       "3                 0.091134  ...               0.598941        0.924412   \n",
       "4                 0.165606  ...               0.498874        0.860891   \n",
       "..                     ...  ...                    ...             ...   \n",
       "471               0.029743  ...               0.586405        0.942469   \n",
       "472               0.076199  ...               0.525921        0.878598   \n",
       "473               0.471745  ...               0.579492        0.907477   \n",
       "474               0.153474  ...               0.871370        0.972082   \n",
       "475               0.133008  ...               0.700830        0.885800   \n",
       "\n",
       "     answer_level_of_information  answer_plausible  answer_relevance  \\\n",
       "0                       0.405140          0.896921          0.905565   \n",
       "1                       0.465002          0.922247          0.947569   \n",
       "2                       0.398799          0.878725          0.878636   \n",
       "3                       0.492892          0.919292          0.951111   \n",
       "4                       0.486585          0.893704          0.893761   \n",
       "..                           ...               ...               ...   \n",
       "471                     0.520021          0.929353          0.965514   \n",
       "472                     0.446159          0.852091          0.878557   \n",
       "473                     0.437612          0.896407          0.921612   \n",
       "474                     0.634717          0.966755          0.977656   \n",
       "475                     0.456123          0.894282          0.896635   \n",
       "\n",
       "     answer_satisfaction  answer_type_instructions  answer_type_procedure  \\\n",
       "0               0.767462                  0.052794               0.054397   \n",
       "1               0.849771                  0.911952               0.116967   \n",
       "2               0.766212                  0.099920               0.054283   \n",
       "3               0.857055                  0.795927               0.139120   \n",
       "4               0.791560                  0.221989               0.092568   \n",
       "..                   ...                       ...                    ...   \n",
       "471             0.887291                  0.935165               0.143282   \n",
       "472             0.805854                  0.764479               0.137364   \n",
       "473             0.811333                  0.531473               0.139557   \n",
       "474             0.937250                  0.131338               0.084272   \n",
       "475             0.809449                  0.604094               0.099218   \n",
       "\n",
       "     answer_type_reason_explanation  answer_well_written  \n",
       "0                          0.879457             0.771410  \n",
       "1                          0.096710             0.719807  \n",
       "2                          0.872505             0.742907  \n",
       "3                          0.649683             0.735062  \n",
       "4                          0.632834             0.747930  \n",
       "..                              ...                  ...  \n",
       "471                        0.079139             0.719842  \n",
       "472                        0.155623             0.689073  \n",
       "473                        0.549874             0.789518  \n",
       "474                        0.861624             0.849877  \n",
       "475                        0.281257             0.672800  \n",
       "\n",
       "[476 rows x 31 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}